import pyaudio
import wave
import os
import random
import gradio as gr
import time
import torch
import gc
import warnings
warnings.filterwarnings('ignore')
from zhconv import convert
from LLM import LLM
from TTS import EdgeTTS
from src.cost_time import calculate_time

from configs import *

default_system = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
#å½•åˆ¶éŸ³é¢‘
voice = 'zh-CN-XiaoxiaoNeural'
# è¯­éŸ³åˆæˆçš„æ–¹æ³•
tts_method == 'Edge-TTS'
rate = 0
# è®¾ç½®é»˜è®¤çš„prompt
prefix_prompt = '''è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'''

edgetts = EdgeTTS()

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
exp_weight = 1
use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5
# éŸ³é‡
volume = 0
pitch = 0

os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"

def record():
    chunk = 1024  # Record in chunks of 1024 samples
    sample_format = pyaudio.paInt16  # 16 bits per sample
    channels = 2
    fs = 44100  # Record at 44100 samples per second
    seconds = 3
    filename = "output.wav"

    p = pyaudio.PyAudio()  # Create an interface to PortAudio

    print('Recording')

    stream = p.open(format=sample_format,
                    channels=channels,
                    rate=fs,
                    frames_per_buffer=chunk,
                    input=True)

    frames = []  # Initialize array to store frames

    # Store data in chunks for 3 seconds
    for i in range(0, int(fs / chunk * seconds)):
        data = stream.read(chunk)
        frames.append(data)

    # Stop and close the stream
    stream.stop_stream()
    stream.close()
    # Terminate the PortAudio interface
    p.terminate()

    print('Finished recording')

    # Save the recorded data as a WAV file
    wf = wave.open(filename, 'wb')
    wf.setnchannels(channels)
    wf.setsampwidth(p.get_sample_size(sample_format))
    wf.setframerate(fs)
    wf.writeframes(b''.join(frames))
    wf.close()

# è¯­éŸ³è½¬æ–‡å­—
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
        gr.Warning(question)
    return question


def Talker_response_img():
    record()
    # ç”¨ FunASRæ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
    asr = FunASR()
    text = asr.transcribe(output.wav)
    # ç”¨Qwenæ¨¡å‹è¿›è¡Œå›å¤
    llm_class = LLM(mode='offline')
    llm = llm_class.init_model('Qwen')
    answer = llm.generate(text, default_system)
    print(answer)
    # å°†ç»“æœè¿›è¡Œè¯­éŸ³åˆæˆ
    driven_audio, driven_vtt = TTS_response(answer, voice, rate, volume, pitch)
    # è¿›è¡Œå›¾åƒçš„é©±åŠ¨
    from TFG import ERNeRF
    nerf = ERNeRF()
    nerf.init_model('./checkpoints/Obama_ave.pth', './checkpoints/Obama.json')
    nerf.predict(driven_audio)
    video = talker.predict(driven_audio)
    return video

def image():
    with gr.Tabs(elem_id="sadtalker_genearted"):
        gen_video = gr.Video(label="æ•°å­—äººè§†é¢‘", format="mp4")
    submit = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')
    submit.click(
        fn=Talker_response_img,
        outputs=[gen_video]

    )
def TTS_response(answer, voice, rate, volume, pitch):
    edgetts.predict(text, voice, rate, volume, pitch, 'answer.wav', 'answer.vtt')
    return 'answer.wav', 'answer.vtt'

if __name__ == "__main__":
    image()
